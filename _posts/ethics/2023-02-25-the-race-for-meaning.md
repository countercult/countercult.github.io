---
layout: page-fullwidth
title:  "The race for meaning"
teaser: "Who are the people racing for meaning?"
breadcrumb: true
categories:
    - ai
    - ethics
permalink: /blog/the-race-for-meaning/
header: no
image:
    title: blog/pexels-yaroslav-danylchenko-4113084.jpg
    frontpage: blog/pexels-yaroslav-danylchenko-4113084.jpg
    
---

If computational semantics inevitably evokes a range of AI applications, AI itself calls for a discussion of the social and economic impact of new technologies. There are hundreds of books and articles doing exactly that, and some prominent examples [have already been mentioned]() in this blog. But while those publications usually promote stories of future doom or prosperity, few provide an insider’s look into the social dynamics and politics of the scientific communities that will supposedly deliver their predicted dystopia (or utopia). The kind of meaning implemented by current AI systems is inextricably linked to the human community that has been developing it over the last ten years or so.


## The Association for Computational Linguistics

Whoever wants to experience the Computational Semantics community should attend one of the annual meetings of the Association for Computational Linguistics (ACL). The main one, simply referred to as ‘the ACL’, takes place yearly, moving its location from America to Europe and finally to Asia, in a three-year rhythm. It is a big affair, with thousands of scientific papers being submitted for peer-reviewing, in the hope of being presented at the conference, and thousands of participants joining the actual event. The meeting is funded by the largest companies in the world. Participants get a tote bag at the registration desk, bearing the names of technology giants such as Google, Apple, Facebook/Meta, Amazon, Microsoft, Baidu and Tencent, but also more traditional companies such as Bloomberg, Bosch and Samsung. Papers are presented in parallel sessions, interrupted by coffee breaks in huge areas that host the stands of both established corporations and new startups, eager to recruit from the ACL population.

The average computational linguist is friendly, informal, and badly dressed. Contrary to what stereotypes about computer scientists might suggest, they talk a lot, ask questions, and take interest in others in a polite and acknowledging way. But the goal of the meeting is clear: competition. The best papers at the ACL will be cited hundreds, sometimes thousands of times, in the years following the conference. What makes them the best? Their performance on predefined tasks, designed to test the ability of systems to simulate linguistic phenomena such as sentence composition, common sense reasoning, sentiment analysis or acceptability judgements. In recent years, the winners of this competition have increasingly come from industrial contexts rather than academia. One positive aspect of this development is that the engineering side of computational linguistics has progressed at an unprecedented rate in the last decade, producing algorithms that are already used in widespread, real-life applications. One negative aspect is the loss of theoretical background in the field. The aims of the ACL in 2022 lie far from the fundamental questions that drove Chomsky, Montague or Masterman. To reflect this, the term ‘Computational Linguistics’ is slowly becoming obsolete, with most engineers preferring to talk of their work as ‘Natural Language Processing’ (NLP).


## Algorithmic access

The move from an essentially academic culture to an industrial one has also come with consequences for algorithmic access, that is, for the ability of an average user to train and deploy computational models. Computational linguistics is a leader in open science: the ACL anthology contains over 74,000 scientific papers freely available online; it is unthinkable to publish in the field without a publicly accessible version of the code and dataset used in one’s experiments. Anybody with some access to the Internet can read about the latest discoveries in the field, and download the cutting-edge AI models crafted by top universities and industrial labs. But can anybody use them to their full capacity?

In October 2021, the ACL published a document calling for more computationally efficient NLP, pointing out that the resources required to run state-of-the-art models are now beyond what many universities, small organisations and/or individuals can afford. The document responded to some growing frustration from parts of the community who feel that, for lack of financial, human and time resources, they cannot compete anymore with large private labs, making their research obsolete or simply unattractive given the current trends. Notably, the statement also raised concern about the environmental cost of linguistic experimentation. This was based on research by Emma Strubell and colleagues, who highlighted in 2019 that training one single state-of-the-art ‘language model’ produced the same quantity of CO2 as five cars’ entire lifetimes, from showroom to scrapeyard.

In the current context, you can download from the Internet some of the most powerful AI algorithms in existence. But most likely, all you can do with them is frame them and put them on your wall. This also goes for the very people who designed those systems. Knowing all the intricacies of a piece of code is of no use without the password to the powerful servers that can run it. And the availability of such computational resources is slowly becoming out of reach for public institutions, but also for small businesses. It is arguable whether really, the current trends in machine learning will lead to a healthy and diverse economic ecosystem.

Given this state of affairs, the ACL’s intervention and its call for more efficient algorithms seems a mature exercise in self-reflection. The main point is: thinking machines can be built in many different ways, and the current one is not viable either for science itself, or for the real-world applications of science. This is a powerful point, and one that is best appreciated by a reader with some intuition of the frameworks underlying different theories of semantics. Do machines have to think the way they think? Which other possibilities are offered by the mathematical space of meaning?
In actual fact, it is unclear whether the ACL’s call for action will change the status quo. To put it simply, someone who has access to a large computer is someone who has access to many small computers. So even if it becomes the norm to build systems for power-efficient hardware, based on a different semantic theory, large entities will have many more of those small systems at their disposal – and thus retain their edge. Tellingly, half of the authors of the ‘Efficient NLP’ document are affiliated with the richest companies in the world (Google and its subsidiary DeepMind, Meta AI, Amazon).


## The winners

So who will churn out more possible worlds per second? Who would have the resources to analyse the properties of those worlds and choose the ones to promote? The current state of the computational linguistics community may seem to confirm the concerns expressed by various commentators on the influence of big tech. Fortunately, understanding meaning is not simply the key to building the perfect AI. It is also the key to the most efficient and democratised computer system out there: the human brain.
